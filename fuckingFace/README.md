## Tokenizer

- 训练一个分词器或者直接读取训练好的分词器
- 依赖文件
    - merges.txt
    - vocab.json